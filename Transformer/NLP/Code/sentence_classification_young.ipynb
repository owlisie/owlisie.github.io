{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac7dfc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a1b8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "lr = 0.001\n",
    "\n",
    "emb_size = 320\n",
    "hidden_size = 320\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02b43155",
   "metadata": {},
   "source": [
    "Sequential sentence data to word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f9512df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(\n",
    "        self, path,\n",
    "        batch_size = 64,\n",
    "        valid_ratio = 0.2,\n",
    "        device = -1,\n",
    "        max_vocab = 50000,\n",
    "        min_freq = 1,\n",
    "        use_eos = False, # not translation : eos unessential\n",
    "        shuffle = True,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.label = data.Field(\n",
    "            sequential=False,\n",
    "            use_vocab=True,\n",
    "            unk_token=None\n",
    "        )\n",
    "\n",
    "        self.text = data.Field(\n",
    "            use_vocab=True,\n",
    "            batch_first=True,\n",
    "            include_lengths=False,\n",
    "            init_token='<cls>', # for classification\n",
    "            eos_token='<EOS>' if use_eos else None,\n",
    "        )\n",
    "\n",
    "        train, valid = data.TabularDataset(\n",
    "            path=path,\n",
    "            format='csv',\n",
    "            skip_header=True, # @young : 'label', 'text' 제거\n",
    "            fields=[\n",
    "                ('label', self.label),\n",
    "                ('text', self.text),\n",
    "            ],\n",
    "        ).split(split_ratio=(1 - valid_ratio))\n",
    "\n",
    "        # batch 단위 iterator for NLP\n",
    "        self.train_loader, self.valid_loader = data.BucketIterator.splits(\n",
    "            (train, valid),\n",
    "            batch_size=batch_size,\n",
    "            device='cuda:0' ,\n",
    "            shuffle=shuffle,\n",
    "            sort_key=lambda x: len(x.text),\n",
    "            sort_within_batch=True,\n",
    "        )\n",
    "\n",
    "        # init word vocab\n",
    "        self.label.build_vocab(train)\n",
    "        self.text.build_vocab(train, max_size=max_vocab, min_freq=min_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7761f9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLoad = DataLoader('../data/test2.csv', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de863dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'4': 0, '5': 1, '3': 2, '2': 3, '1': 4})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = dataLoad.label\n",
    "text = dataLoad.text\n",
    "\n",
    "train_loader = dataLoad.train_loader\n",
    "val_loader = dataLoad.valid_loader\n",
    "\n",
    "label.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b890f941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'4': 3, '5': 4, '3': 2, '2': 1, '1': 0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set vocab label in order\n",
    "label.vocab.stoi['1'] = 0\n",
    "label.vocab.stoi['2'] = 1\n",
    "label.vocab.stoi['3'] = 2\n",
    "label.vocab.stoi['4'] = 3\n",
    "label.vocab.stoi['5'] = 4\n",
    "label.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec84d8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'freqs': Counter({'5': 15899, '4': 16738, '3': 9986, '2': 5555, '1': 1964}),\n",
       " 'itos': ['4', '5', '3', '2', '1'],\n",
       " 'unk_index': None,\n",
       " 'stoi': defaultdict(None, {'4': 3, '5': 4, '3': 2, '2': 1, '1': 0}),\n",
       " 'vectors': None}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check label distribution\n",
    "vars(label.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b5ed707",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(text.vocab.stoi)\n",
    "n_cls = len(label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ce9aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weight\n",
    "def save_checkpoint(model, pth):\n",
    "    torch.save(model.state_dict(), pth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d3cbb99",
   "metadata": {},
   "source": [
    "Sequential Multi Head Attention for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttn(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size,\n",
    "        emb_size,\n",
    "        n_layers = 4,\n",
    "        dropout_rate = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size # hidden embedding size\n",
    "        self.emb_size = emb_size # word embedding vector size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.n_heads = 5\n",
    "        self.n_hid = hidden_size//self.n_heads # hidden size for each head\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.n_hid])).to(device)\n",
    "\n",
    "        # init q, k, v with linear layer\n",
    "        self.mlp_q = nn.Linear(self.emb_size, self.hidden_size, bias=False)\n",
    "        self.mlp_k = nn.Linear(self.emb_size, self.hidden_size, bias=False)\n",
    "        self.mlp_v = nn.Linear(self.emb_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.mlp_x = nn.Linear(self.hidden_size, self.emb_size, bias=False)\n",
    "\n",
    "        self.sm = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None): # [batch, n+1->n, hidden_size]\n",
    "        # init Query, Key and Value\n",
    "        x_q = self.mlp_q(x) # [batch, n, hidden_size = n_heads*n_hid]\n",
    "        x_k = self.mlp_k(x) # [batch, n, hidden_size = n_heads*n_hid]\n",
    "        x_v = self.mlp_v(x) # [batch, n, hidden_size = n_heads*n_hid]\n",
    "\n",
    "        # multi-head to Q, K and V\n",
    "        x_q = x_q.view(x.shape[0], -1, self.n_heads, self.n_hid).permute(0, 2, 1, 3).contiguous() # [batch, n_heads, n, h_hid]\n",
    "        x_k = x_k.view(x.shape[0], -1, self.n_heads, self.n_hid).permute(0, 2, 1, 3).contiguous() # [batch, n_heads, n, h_hid]\n",
    "        x_v = x_v.view(x.shape[0], -1, self.n_heads, self.n_hid).permute(0, 2, 1, 3).contiguous() # [batch, n_heads, n, h_hid]\n",
    "\n",
    "        x_q = x_q.reshape(x.shape[0]*self.n_heads, -1, self.n_hid) # [batch*n_heads, n, n_hid]\n",
    "        x_k = x_k.reshape(x.shape[0]*self.n_heads, -1, self.n_hid) # [batch*n_heads, n, n_hid]\n",
    "        x_v = x_v.reshape(x.shape[0]*self.n_heads, -1, self.n_hid) # [batch*n_heads, n, n_hid]\n",
    "\n",
    "        # Attention Weight with Q & K\n",
    "        w = torch.bmm(x_q, x_k.transpose(2, 1))/self.scale # [batch*n_heads, n, n]\n",
    "\n",
    "        if mask != None:\n",
    "            w = w.masked_fill(mask == 0, -1e10) # [batch*n_heads, n, n]\n",
    "\n",
    "        w = self.sm(w) # [batch*n_heads, n, n]\n",
    "        # w = nn.functional.dropout(w, self.dropout_rate)\n",
    "\n",
    "        # Multiply attention weight with V\n",
    "        attn = torch.bmm(w, x_v) # [batch*n_heads, n, n_hid]\n",
    "        x = attn.view(x.shape[0], self.n_heads, -1, self.n_hid) # [batch, n_heads, n, n_hid]\n",
    "        x = x.permute(0, 2, 1, 3) # [batch, n, n_heads, n_hid]\n",
    "    \n",
    "        # Concat multi-head attention outcomes\n",
    "        x = x.reshape(x.shape[0], -1, self.hidden_size) # [batch, n, hidden_size]\n",
    "        x = self.mlp_x(x) # [batch, n, hidden_size]\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8da73680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        emb_size,\n",
    "        n_classes,\n",
    "        n_layers=4,\n",
    "        dropout_rate=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb_size = emb_size\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.n_heads = 5\n",
    "        self.n_tran = 4\n",
    "        self.max_len = 2000 # average length 사용???? padding이 많이 들어갈수록 연산?? 만 많아지나??\n",
    "\n",
    "        self.emb = nn.Embedding(self.input_size, self.emb_size)\n",
    "        self.pemb = nn.Embedding(self.max_len, self.emb_size)\n",
    "\n",
    "        self.attns = nn.ModuleList([MultiHeadAttn(hidden_size=self.hidden_size, emb_size=self.hidden_size) for _ in range(self.n_tran)])\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.n_classes)\n",
    "        self.actv = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x): # [batch, n+1]\n",
    "        # init mask for label <pad> (1)\n",
    "        mask = (x != 1).unsqueeze(-1) # [batch, n+1, 1]\n",
    "        mask = torch.bmm(mask.type(torch.LongTensor), mask.transpose(2, 1).type(torch.LongTensor)).unsqueeze(1) # [batch, 1, n+1, n+1]\n",
    "        mask = mask.repeat(1, self.n_heads, 1, 1) # [batch, n_heads, n+1, n+1]\n",
    "        mask = mask.view(mask.shape[0]*mask.shape[1], mask.shape[2], mask.shape[3]).to(device) # [batch*n_heads, n+1, n+1]\n",
    "\n",
    "        # word embedding\n",
    "        x_emb = self.emb(x) # [batch, n+1, hidden_size]\n",
    "\n",
    "        # init position embedding for sequence\n",
    "        pos = torch.arange(x.shape[1]).unsqueeze(0).repeat(x.shape[0], 1).to(device) #.expand(x_emb.shape[0], x_emb.shape[1]).to(device) # [batch, n+1]\n",
    "        x_pos = self.pemb(pos).to(device) # [batch, n+1, hidden_size]\n",
    "        x = x_emb # [batch, n+1, hidden_size]\n",
    "\n",
    "        # init classification token\n",
    "        # cls_token = torch.zeros((x.shape[0], 1)).type(torch.LongTensor).to(device) # [batch, 1]\n",
    "        # cls_token = self.cls(cls_token).to(device) # [batch, 1, hidden_size]\n",
    "        # x = torch.cat((cls_token, x), 1).to(device) # [batch, n+1, hidden_size]\n",
    "\n",
    "        for idx in range(self.n_tran):\n",
    "            x = x + x_pos\n",
    "            x = self.attns[idx](x, mask=mask) # [batch, n+1, hidden_size]\n",
    "\n",
    "        # time step n to 1\n",
    "        x = self.fc(x[:, 0, :]) # [batch, n_class] [batch, 1, hidden_size]\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bf43f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SeqClassifier(input_size=len(text.vocab.stoi),\n",
    "                      emb_size=emb_size,\n",
    "                      hidden_size=hidden_size,\n",
    "                      n_classes=n_cls,\n",
    "                      n_layers=4)\n",
    "\n",
    "model.to(device)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "crit.to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b91990",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    model.train()\n",
    "    train_acc, train_cnt = 0, 0\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        x, y = batch.text.to(device), batch.label.to(device)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        ans = model(x)\n",
    "\n",
    "        loss = crit(ans, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_acc += (ans.argmax(1) == y).sum().item()\n",
    "        train_cnt += y.size(0)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_cnt = 0, 0\n",
    "\n",
    "        for batch in val_loader:\n",
    "            x, y = batch.text.to(device), batch.label.to(device)\n",
    "\n",
    "            ans = model(x)\n",
    " \n",
    "            loss = crit(ans, y)\n",
    "\n",
    "            val_acc += (ans.argmax(1) == y).sum().item()\n",
    "            val_cnt += y.size(0)\n",
    "\n",
    "    print(ans.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "    print('Epoch : {:3d}/{} Loss : {:.4f} TrainAcc : {:.4f} ValAcc : {:.4f}'.format(i + 1, epoch, loss, train_acc/train_cnt, val_acc/val_cnt))\n",
    "\n",
    "# save_checkpoint(model, './model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b5d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0525cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37075f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082fcc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987be2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdcbe4c587a8000a1343b228a1562601d1a8b44cc2ab784ff291fe50eaaf3ab2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
